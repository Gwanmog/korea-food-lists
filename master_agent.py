import os
import requests
import time
import random
import csv
from dotenv import load_dotenv

from naver_agent import search_naver_blogs, scrape_naver_blog_text
from critic_agent import evaluate_restaurant, get_kakao_categories

# Pathing setup
script_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(script_dir, 'soul-food-api', '.env')
load_dotenv(dotenv_path=env_path)

KAKAO_API_KEY = os.getenv("KAKAO_REST_API_KEY")

# ==========================================
# âš™ï¸ THE SEOUL MASTER QUEUE (ALL 25 DISTRICTS)
# ==========================================
NEIGHBORHOODS = [
    # --- Mapo / Seodaemun (The Indie & University Belt) ---
    "ì„œêµë™",  # Hongdae (Official address for the main strip)
    "ì°½ì²œë™",  # Sinchon (Official address)
    "ëŒ€í˜„ë™",  # Edae
    "ì—°ë‚¨ë™",  # Yeonnam
    "ë§ì›ë™",  # Mangwon
    "í•©ì •ë™",  # Hapjeong
    "ìƒìˆ˜ë™",  # Sangsu
    "ê³µë•ë™",  # Gongdeok
    "ìƒì•”ë™",  # Digital Media City (DMC)

    # --- Yongsan (The International & Craft Beer Hub) ---
    "ì´íƒœì›ë™",  # Itaewon
    "ìš©ì‚°ë™2ê°€",  # Haebangchon (Official address)

    # --- Jongno / Jung-gu (Historical & Downtown) ---
    "ìµì„ ë™",  # Ikseondong
    "ì‚¼ì²­ë™",  # Samcheong
    "ì„ì§€ë¡œ",  # Euljiro
    "ëª…ë™",  # Myeongdong
    "ì‹ ë‹¹ë™",  # Sindang
    "ì°½ì‹ ë™",  # Changsin

    # âš ï¸ High-Density Downtown Splintering
    "ì¢…ë¡œ3ê°€",  # Jongno 3-ga
    "ëˆì˜ë™",  # Jongno 3-ga (Alleyways often register here)
    "ë‚™ì›ë™",  # Jongno 3-ga (Northern edge)
    "ì¶©ë¬´ë¡œ",  # Chungmuro
    "í•„ë™",  # Chungmuro (Many spots register under this address)
    "ê´‘í¬ë™",  # Dongdaemun
    "ì„ì§€ë¡œ6ê°€",  # Dongdaemun (DDP area)

    # --- Gangnam / Seongdong (Trendy & Commercial) ---
    "ì—­ì‚¼ë™",  # Gangnam (East of the station)
    "ì„œì´ˆë™",  # Gangnam (West of the station)
    "ì••êµ¬ì •ë™",  # Apgujeong
    "ì‹ ì‚¬ë™",  # Sinsa
    "ì„±ìˆ˜ë™",  # Seongsu
    "ë§ˆì¥ë™",  # Majang

    # --- Yeongdeungpo / Seongbuk ---
    "ë¬¸ë˜ë™",  # Mullae
    "ì •ë¦‰ë™"  # Jeongneung
]
# ğŸ¯ THE TARGET DICTIONARY
# Format: "Kakao Search Bait": ("Gemini Master Target", Strict_Mode_Boolean)
KEYWORDS = {
    # The Beer Core
    "ìˆ˜ì œë§¥ì£¼": ("ìˆ˜ì œë§¥ì£¼", False),
    "í¬ë˜í”„íŠ¸ë¹„ì–´": ("ìˆ˜ì œë§¥ì£¼", False),
    "í¬ë˜í”„íŠ¸": ("ìˆ˜ì œë§¥ì£¼", False),
    "íƒ­í•˜ìš°ìŠ¤": ("ìˆ˜ì œë§¥ì£¼", False),
    "ì—ì¼": ("ìˆ˜ì œë§¥ì£¼", False),
    "IPA": ("ìˆ˜ì œë§¥ì£¼", False),

    # The Mixed/Traditional Core
    "ì–‘ì¡°ì¥": ("ì–‘ì¡°ì¥", False),
    "ë§‰ê±¸ë¦¬": ("ë§‰ê±¸ë¦¬", False),
    "ì „í†µì£¼": ("ë§‰ê±¸ë¦¬", False)
}

MAX_PLACES_PER_SEARCH = 45
CSV_FILENAME = os.path.join(script_dir, 'neon_guide_review_queue.csv')

# ==========================================

def discover_restaurants(keyword, location, max_results):
    """
    Paginated Discovery Agent: Sweeps up to 3 pages (45 results)
    to prevent true hotspots from being buried by Kakao's SEO keyword ranking.
    """
    print(f"\nğŸ—ºï¸ Discovery Agent: Deep-sweeping Kakao for '{location} {keyword}'...")
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}

    all_places = []

    # Sweep Page 1, Page 2, and Page 3 (15 items per page)
    for page in range(1, 4):
        params = {
            "query": f"{location} {keyword}",
            "size": 15, # Kakao's strict limit per page
            "page": page
        }

        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                documents = data.get('documents', [])
                all_places.extend(documents)

                # If we hit the end of Kakao's database before page 3, break early
                if data.get('meta', {}).get('is_end', True):
                    break
            else:
                print(f"âŒ Kakao API Error on page {page}: {response.status_code}")
                break
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Network error connecting to Kakao: {e}")
            break

    print(f"âœ… Recovered {len(all_places)} spots from the Kakao depths.")
    return all_places[:max_results]


def append_to_csv(row_dict):
    """LIVE CHECKPOINTING: Saves one row to the CSV immediately."""
    headers = ["Neighborhood", "Keyword", "Restaurant Name", "Score", "Award Level", "AI Justification", "English Desc",
               "Korean Desc", "Kakao URL", "Lat", "Lon"]

    file_exists = os.path.isfile(CSV_FILENAME)

    with open(CSV_FILENAME, 'a', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        if not file_exists:
            writer.writeheader()
        writer.writerow(row_dict)


def load_existing_restaurants():
    """Reads the CSV to memorize places we've already scored across multiple runs."""
    seen_names = set()
    if os.path.isfile(CSV_FILENAME):
        with open(CSV_FILENAME, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                seen_names.add(row.get("Restaurant Name"))
    return seen_names


def is_strong_hit(place, keyword, valid_categories, expected_neighborhood):
    """
    Agile pre-filter powered by the AI Coordinator and Geographic bounds.
    """
    name = place.get('place_name', '')
    category = place.get('category_name', '')
    address = place.get('address_name', '')

    # ğŸš¨ THE GEOGRAPHIC BOUNCER
    # If the district we searched for isn't in the official address, Kakao is bleeding over.
    if expected_neighborhood not in address:
        return False

    # 1. Direct hit in the restaurant's name
    if keyword in name:
        return True

    # 2. Direct hit in Kakao's default category
    if keyword in category:
        return True

    # 3. Dynamic check against the AI's allowed categories
    if any(valid_cat in category for valid_cat in valid_categories):
        return True

    return False


def run_massive_pipeline():
    seen_places = load_existing_restaurants()

    # ğŸ”„ Unpack all three variables!
    for search_bait, (master_target, is_strict) in KEYWORDS.items():
        print(f"\n" + "*" * 50)
        print(f"ğŸ¯ BAIT: {search_bait} | TARGET: {master_target} | STRICT: {is_strict}")
        print(f"*" * 50)

        # Pass the strict flag to the Coordinator
        valid_categories = get_kakao_categories(search_bait, strict_mode=is_strict)

        for neighborhood in NEIGHBORHOODS:
            print(f"\nğŸ“ INITIATING SECTOR SCAN: {neighborhood} ({search_bait})")

            # Use SEARCH BAIT for Kakao
            places_to_investigate = discover_restaurants(search_bait, neighborhood, MAX_PLACES_PER_SEARCH)

            for place in places_to_investigate:
                restaurant_name = place['place_name']

                # ğŸš€ Pass 'neighborhood' to the Bouncer and print the result!
                if not is_strong_hit(place, search_bait, valid_categories, neighborhood):
                    print(f"â­ï¸ Bouncing {restaurant_name} (Category or Geography mismatch).")
                    continue

                if restaurant_name in seen_places:
                    print(f"â­ï¸ Skipping {restaurant_name} (Already in CSV).")
                    continue

                seen_places.add(restaurant_name)
                # FIX 1: Use search_bait instead of keyword in the print statement
                print(f"\nğŸ•µï¸ Investigating: {restaurant_name} ({neighborhood} / {search_bait})")

                # --- A. Get Naver Blogs ---
                blog_results = search_naver_blogs(restaurant_name, neighborhood)
                if not blog_results:
                    continue

                # ğŸš€ THE FAST-PASS FILTER ğŸš€
                # Check if the target vibe is even mentioned in the blog titles/snippets
                # If we are looking for craft beer, we look for key terms.
                fast_pass_terms = ["ìˆ˜ì œë§¥ì£¼", "í¬ë˜í”„íŠ¸", "ë¸Œë£¨ì–´ë¦¬", "ì–‘ì¡°ì¥", "ì—ì¼", "IPA"]

                passed_fast_pass = False
                for blog in blog_results:
                    title = blog.get('title', '')
                    snippet = blog.get('description', '')

                    if any(term in title or term in snippet for term in fast_pass_terms):
                        passed_fast_pass = True
                        break  # We found proof! Stop checking snippets.

                if not passed_fast_pass:
                    print(
                        f"â­ï¸ Fast-Pass Failed: {restaurant_name}. No mention of target keywords in top 10 blog titles. Skipping AI.")
                    continue
                # ğŸš€ ------------------------ ğŸš€

                print(f"âœ… Fast-Pass Passed! Scraping full blogs for {restaurant_name}...")

                scraped_texts = []
                for blog in blog_results:
                    url = blog['link']
                    text = scrape_naver_blog_text(url)
                    if text:
                        scraped_texts.append(text)

                    # Human Jitter
                    time.sleep(random.uniform(1.5, 3.2))

                if not scraped_texts:
                    print("âš ï¸ Not enough readable data. Skipping.")
                    continue

                # --- B. Send to Gemini for Scoring ---
                evaluation = evaluate_restaurant(restaurant_name, scraped_texts, master_target)

                # --- C. Live Save to Staging Queue ---
                if evaluation:
                    score = evaluation.get('score', 0)
                    print(f"ğŸ¯ AI Scored {restaurant_name}: {score}/100")

                    row = {
                        "Neighborhood": neighborhood,
                        # FIX 2: Save it under the master_target so your lists stay clean!
                        "Keyword": master_target,
                        "Restaurant Name": restaurant_name,
                        "Score": score,
                        "Award Level": evaluation.get('award_level', 'None'),
                        "AI Justification": evaluation.get('justification', ''),
                        "English Desc": evaluation.get('description_en', ''),
                        "Korean Desc": evaluation.get('description_ko', ''),
                        "Kakao URL": place.get('place_url', ''),
                        "Lat": place.get('y', ''),
                        "Lon": place.get('x', '')
                    }

                    append_to_csv(row)
                else:
                    print(f"âŒ AI failed to evaluate {restaurant_name}.")

                time.sleep(random.uniform(4.0, 7.0))

    print(f"\nğŸ Massive Sweep Complete! Data safely secured in {CSV_FILENAME}.")


if __name__ == "__main__":
    run_massive_pipeline()
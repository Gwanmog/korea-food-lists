import os
import requests
import time
import random
import csv
from dotenv import load_dotenv

from naver_agent import search_naver_blogs, scrape_naver_blog_text
from critic_agent import evaluate_restaurant, get_kakao_categories

# Pathing setup
script_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(script_dir, 'soul-food-api', '.env')
load_dotenv(dotenv_path=env_path)

KAKAO_API_KEY = os.getenv("KAKAO_REST_API_KEY")

# ==========================================
# âš™ï¸ THE SEOUL MASTER QUEUE (ALL 25 DISTRICTS)
# ==========================================
NEIGHBORHOODS = [
    "ì„œêµë™", "ì°½ì²œë™", "ëŒ€í˜„ë™", "ì—°ë‚¨ë™", "ë§ì›ë™", "í•©ì •ë™", "ìƒìˆ˜ë™", "ê³µë•ë™", "ìƒì•”ë™",
    "ì´íƒœì›ë™", "ìš©ì‚°ë™2ê°€",
    "ê¶Œë†ë™", "ìµì„ ë™", "ì‚¼ì²­ë™", "ì„ì§€ë¡œ", "ëª…ë™", "ì‹ ë‹¹ë™", "ì°½ì‹ ë™",
    "ì¢…ë¡œ3ê°€", "ëˆì˜ë™", "ë‚™ì›ë™", "ì¶©ë¬´ë¡œ", "í•„ë™", "ê´‘í¬ë™", "ì„ì§€ë¡œ6ê°€",
    "ì—­ì‚¼ë™", "ì„œì´ˆë™", "ì••êµ¬ì •ë™", "ì‹ ì‚¬ë™", "ì„±ìˆ˜ë™", "ë§ˆì¥ë™",
    "ë¬¸ë˜ë™", "ì •ë¦‰ë™"
]
# ğŸ¯ THE TARGET DICTIONARY
# Format: "Kakao Search Bait": ("Gemini Master Target", Strict_Mode_Boolean)
KEYWORDS = {
    # The Fried Chicken Essentials
    "ì¹˜í‚¨": ("ì¹˜í‚¨", False),
    "ë‹­ê°•ì •": ("ì¹˜í‚¨", False),
    "ì–‘ë…ì¹˜í‚¨": ("ì¹˜í‚¨", False),

    # The Casual Street Food & Market Snacking
    "ë–¡ë³¶ì´": ("ë¶„ì‹", False),
    "ê¹€ë°¥": ("ë¶„ì‹", False),
    "íŠ€ê¹€": ("ë¶„ì‹", False),
    "í˜¸ë–¡": ("ë””ì €íŠ¸", False),
    "ë¹ˆëŒ€ë–¡": ("ì „", False),
    "ì˜¤ë…": ("ë¶„ì‹", False),

    # Hangover & Soul Food (The Working-Class Heroes)
    "êµ­ë°¥": ("êµ­ë°¥", False),
    "ê°ìíƒ•": ("ê°ìíƒ•", False),
    "ì œìœ¡ë³¶ìŒ": ("ë°±ë°˜", False),

    # The Spring Seasonal Exclusive
    "ì­ˆê¾¸ë¯¸": ("í•´ì‚°ë¬¼", False),

    # Soju Tents & Late Night
    "ê³±ì°½": ("ê³±ì°½", False),
    "ìœ¡íšŒ": ("ìœ¡íšŒ", False),

    # Trendy Desserts
    "ë‘ë°”ì´ ì´ˆì½œë¦¿": ("ë””ì €íŠ¸", False)
}

MAX_PLACES_PER_SEARCH = 45
CSV_FILENAME = os.path.join(script_dir, 'neon_guide_review_queue.csv')

# ==========================================

def discover_restaurants(keyword, location, max_results):
    """
    Paginated Discovery Agent: Sweeps up to 3 pages (45 results)
    to prevent true hotspots from being buried by Kakao's SEO keyword ranking.
    """
    print(f"\nğŸ—ºï¸ Discovery Agent: Deep-sweeping Kakao for '{location} {keyword}'...")
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}

    all_places = []

    # Sweep Page 1, Page 2, and Page 3 (15 items per page)
    for page in range(1, 4):
        params = {
            "query": f"{location} {keyword}",
            "size": 15, # Kakao's strict limit per page
            "page": page
        }

        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                documents = data.get('documents', [])
                all_places.extend(documents)

                # If we hit the end of Kakao's database before page 3, break early
                if data.get('meta', {}).get('is_end', True):
                    break
            else:
                print(f"âŒ Kakao API Error on page {page}: {response.status_code}")
                break
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Network error connecting to Kakao: {e}")
            break

    print(f"âœ… Recovered {len(all_places)} spots from the Kakao depths.")
    return all_places[:max_results]


def append_to_csv(row_dict):
    """LIVE CHECKPOINTING: Saves one row to the CSV immediately."""
    headers = ["Neighborhood", "Keyword", "Restaurant Name", "Score", "Award Level", "AI Justification", "English Desc",
               "Korean Desc", "Kakao URL", "Lat", "Lon"]

    file_exists = os.path.isfile(CSV_FILENAME)

    with open(CSV_FILENAME, 'a', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        if not file_exists:
            writer.writeheader()
        writer.writerow(row_dict)


def load_existing_restaurants():
    """Reads the CSV to memorize places we've already scored across multiple runs."""
    seen_names = set()
    if os.path.isfile(CSV_FILENAME):
        with open(CSV_FILENAME, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                seen_names.add(row.get("Restaurant Name"))
    return seen_names


def is_strong_hit(place, keyword, valid_categories, expected_neighborhood):
    """
    Agile pre-filter powered ONLY by Geographic bounds.
    Trusts the Naver Fast-Pass to filter generic bars,
    and the AI Lie Detector to penalize corporate franchises.
    """
    address = place.get('address_name', '')

    # ğŸš¨ THE GEOGRAPHIC BOUNCER (The only rule we need here!)
    if expected_neighborhood not in address:
        return False

    # If it is physically inside the neighborhood, let the AI pipeline judge it!
    return True


def run_massive_pipeline():
    seen_places = load_existing_restaurants()

    # ğŸ”„ Unpack all three variables!
    for search_bait, (master_target, is_strict) in KEYWORDS.items():
        print(f"\n" + "*" * 50)
        print(f"ğŸ¯ BAIT: {search_bait} | TARGET: {master_target} | STRICT: {is_strict}")
        print(f"*" * 50)

        # Pass the strict flag to the Coordinator
        valid_categories = get_kakao_categories(search_bait, strict_mode=is_strict)

        for neighborhood in NEIGHBORHOODS:
            print(f"\nğŸ“ INITIATING SECTOR SCAN: {neighborhood} ({search_bait})")

            # Use SEARCH BAIT for Kakao
            places_to_investigate = discover_restaurants(search_bait, neighborhood, MAX_PLACES_PER_SEARCH)

            for place in places_to_investigate:
                restaurant_name = place['place_name']

                # ğŸš€ Pass 'neighborhood' to the Bouncer and print the result!
                if not is_strong_hit(place, search_bait, valid_categories, neighborhood):
                    print(f"â­ï¸ Bouncing {restaurant_name} (Category or Geography mismatch).")
                    continue

                if restaurant_name in seen_places:
                    print(f"â­ï¸ Skipping {restaurant_name} (Already in CSV).")
                    continue

                seen_places.add(restaurant_name)
                # FIX 1: Use search_bait instead of keyword in the print statement
                print(f"\nğŸ•µï¸ Investigating: {restaurant_name} ({neighborhood} / {search_bait})")

                # --- A. Get Naver Blogs ---
                blog_results = search_naver_blogs(restaurant_name, neighborhood)
                if not blog_results:
                    continue

                # ğŸš€ THE FAST-PASS FILTER ğŸš€
                # Check if the target vibe is even mentioned in the blog titles/snippets
                # If we are looking for craft beer, we look for key terms.
                fast_pass_terms = ["ìˆ˜ì œë§¥ì£¼", "í¬ë˜í”„íŠ¸", "ë¸Œë£¨ì–´ë¦¬", "ì–‘ì¡°ì¥", "ì—ì¼", "IPA"]

                passed_fast_pass = False
                for blog in blog_results:
                    title = blog.get('title', '')
                    snippet = blog.get('description', '')

                    if any(term in title or term in snippet for term in fast_pass_terms):
                        passed_fast_pass = True
                        break  # We found proof! Stop checking snippets.

                if not passed_fast_pass:
                    print(
                        f"â­ï¸ Fast-Pass Failed: {restaurant_name}. No mention of target keywords in top 10 blog titles. Skipping AI.")
                    continue
                # ğŸš€ ------------------------ ğŸš€

                print(f"âœ… Fast-Pass Passed! Scraping full blogs for {restaurant_name}...")

                scraped_texts = []
                for blog in blog_results:
                    url = blog['link']
                    text = scrape_naver_blog_text(url)
                    if text:
                        scraped_texts.append(text)

                    # Human Jitter
                    time.sleep(random.uniform(1.5, 3.2))

                if not scraped_texts:
                    print("âš ï¸ Not enough readable data. Skipping.")
                    continue

                # --- B. Send to Gemini for Scoring ---
                evaluation = evaluate_restaurant(restaurant_name, scraped_texts, master_target)

                # --- C. Live Save to Staging Queue ---
                if evaluation:
                    score = evaluation.get('score', 0)
                    print(f"ğŸ¯ AI Scored {restaurant_name}: {score}/100")

                    row = {
                        "Neighborhood": neighborhood,
                        # FIX 2: Save it under the master_target so your lists stay clean!
                        "Keyword": master_target,
                        "Restaurant Name": restaurant_name,
                        "Score": score,
                        "Award Level": evaluation.get('award_level', 'None'),
                        "AI Justification": evaluation.get('justification', ''),
                        "English Desc": evaluation.get('description_en', ''),
                        "Korean Desc": evaluation.get('description_ko', ''),
                        "Kakao URL": place.get('place_url', ''),
                        "Lat": place.get('y', ''),
                        "Lon": place.get('x', '')
                    }

                    append_to_csv(row)
                else:
                    print(f"âŒ AI failed to evaluate {restaurant_name}.")

                time.sleep(random.uniform(4.0, 7.0))

    print(f"\nğŸ Massive Sweep Complete! Data safely secured in {CSV_FILENAME}.")


if __name__ == "__main__":
    run_massive_pipeline()